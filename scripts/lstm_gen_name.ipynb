{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "\n",
    "from utils import load_dergs\n",
    "from derg import DERG, KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 69\n"
     ]
    }
   ],
   "source": [
    "WORD_START = '<start>'\n",
    "WORD_END = '<end>'\n",
    "WORD_UNKNOWN = '<unknown>'\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx[WORD_UNKNOWN]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.add_word(WORD_START)\n",
    "vocab.add_word(WORD_END)\n",
    "vocab.add_word(WORD_UNKNOWN)\n",
    "for c in string.ascii_letters + string.digits + \"<>$_\":\n",
    "    vocab.add_word(c)\n",
    "    \n",
    "print(\"Total vocabulary size: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718\n"
     ]
    }
   ],
   "source": [
    "dergs = load_dergs('/home/liyc/data/dedroid/dergs/', 'obfuscate_derg.json')\n",
    "print(len(dergs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114022\n"
     ]
    }
   ],
   "source": [
    "def load_node_to_kgid_mapping(node_to_kgid_mapping_path):\n",
    "    f = open(node_to_kgid_mapping_path)\n",
    "    mapping = {}\n",
    "    for line in f.readlines()[1:]:\n",
    "        segs = line.split()\n",
    "        node_global_name = segs[0]\n",
    "        kgid = int(segs[1])\n",
    "        mapping[node_global_name] = kgid\n",
    "    return mapping\n",
    "\n",
    "node_to_kgids = load_node_to_kgid_mapping('/home/liyc/data/dedroid/kg/entity2id.txt')\n",
    "print(len(node_to_kgids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114022\n"
     ]
    }
   ],
   "source": [
    "def load_kgid_embeddings(kgid_embedding_path):\n",
    "    embeddings = json.load(open(kgid_embedding_path))\n",
    "    return embeddings['ent_embeddings']\n",
    "\n",
    "kgid_embeddings = load_kgid_embeddings('/home/liyc/data/dedroid/kg/embedding_HolE_50/embedding.vec.json')\n",
    "print(len(kgid_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646 apps for training, 72 apps for predicting\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def split_train_and_test(dergs, train_portion = 0.9):\n",
    "    random.shuffle(dergs)\n",
    "    sep = int(len(dergs) * train_portion)\n",
    "    train_dergs = dergs[:sep]\n",
    "    test_dergs = dergs[sep:]\n",
    "    return train_dergs, test_dergs\n",
    "\n",
    "train_dergs, test_dergs = split_train_and_test(dergs)\n",
    "print(\"%d apps for training, %d apps for predicting\" % (len(train_dergs), len(test_dergs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeDroidDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset compatible with torch.utils.data.DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, dergs, node_to_kgids, kgid_embeddings, vocab):\n",
    "        self.dergs = dergs\n",
    "        self.node_to_names = []\n",
    "        for g in dergs:\n",
    "            self.node_to_names.extend(g.get_kg_mappings())\n",
    "        self.node_to_kgids = node_to_kgids\n",
    "        self.kgid_embeddings = kgid_embeddings\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (embedding and name).\"\"\"\n",
    "        node_global_name, node_name = self.node_to_names[index]\n",
    "        \n",
    "        # Temporaral work around\n",
    "        node_global_name = node_global_name[16:]\n",
    "        \n",
    "        kgid = self.node_to_kgids[node_global_name]\n",
    "        embedding = self.kgid_embeddings[kgid]\n",
    "\n",
    "        # Convert node name (string) to word ids.\n",
    "        name_vec = []\n",
    "        name_vec.append(vocab(WORD_START))\n",
    "        name_vec.extend([vocab(c) for c in node_name])\n",
    "        name_vec.append(vocab(WORD_END))\n",
    "        \n",
    "        embedding = torch.Tensor(embedding)\n",
    "        target = torch.Tensor(name_vec)\n",
    "        return embedding, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.node_to_names)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (embedding, name).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging name (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (embedding, name). \n",
    "            - embedding: torch tensor of shape (embed_size).\n",
    "            - name: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        embeddings: torch tensor of shape (batch_size, embed_size).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded name.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    embeddings, names = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 1D tensor to 2D tensor).\n",
    "    embeddings = torch.stack(embeddings, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(name) for name in names]\n",
    "    targets = torch.zeros(len(names), max(lengths)).long()\n",
    "    for i, name in enumerate(names):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = name[:end]\n",
    "    return embeddings, targets, lengths\n",
    "\n",
    "\n",
    "def get_loader(dergs, node_to_kgids, kgid_embeddings, vocab, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom DeDroid dataset.\"\"\"\n",
    "    # DeDroid dataset\n",
    "    dedroid_data = DeDroidDataset(dergs, node_to_kgids, kgid_embeddings, vocab)\n",
    "    \n",
    "    # Data loader for DeDroid dataset\n",
    "    # This will return (embeddings, names, lengths) for every iteration.\n",
    "    # embeddings: tensor of shape (batch_size, embed_size).\n",
    "    # names: tensor of shape (batch_size, padded_length).\n",
    "    # lengths: list indicating valid length for each name. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dedroid_data, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "    \n",
    "    \n",
    "class DeDroidRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DeDroidRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, embeddings, names, lengths):\n",
    "        \"\"\"Decode kg embeddings and generate names.\"\"\"\n",
    "        name_embeddings = self.embed(names)\n",
    "        embeddings = torch.cat((embeddings.unsqueeze(1), name_embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, embeddings, states=None):\n",
    "        \"\"\"Samples names for given kg embeddings (Greedy search).\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = embeddings.unsqueeze(1)\n",
    "        for i in range(50):                                      # maximum sampling length\n",
    "            hiddens, states = self.lstm(inputs, states)          # (batch_size, 1, hidden_size), \n",
    "            outputs = self.linear(hiddens.squeeze(1))            # (batch_size, vocab_size)\n",
    "            predicted = outputs.max(1)[1]\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)                         # (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # (batch_size, 20)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "embed_size = 50\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "lr = 0.005\n",
    "num_epochs = 5\n",
    "log_step = 100\n",
    "save_step = 1000\n",
    "model_path = '/home/liyc/data/dedroid/model'\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "# Build data loader\n",
    "data_loader = get_loader(\n",
    "    dergs = train_dergs,\n",
    "    node_to_kgids = node_to_kgids,\n",
    "    kgid_embeddings = kgid_embeddings,\n",
    "    vocab = vocab,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = shuffle,\n",
    "    num_workers = num_workers) \n",
    "\n",
    "# Build the models\n",
    "dedroid_rnn = DeDroidRNN(\n",
    "    embed_size = embed_size,\n",
    "    hidden_size = hidden_size,\n",
    "    vocab_size = len(vocab),\n",
    "    num_layers = num_layers)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dedroid_rnn.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(dedroid_rnn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "# Train the Models\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (embeddings, names, lengths) in enumerate(data_loader):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        embeddings = to_var(embeddings, volatile=False)\n",
    "        names = to_var(names)\n",
    "        targets = pack_padded_sequence(names, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, Backward and Optimize\n",
    "        dedroid_rnn.zero_grad()\n",
    "        outputs = dedroid_rnn(embeddings, names, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % log_step == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f'\n",
    "                  % (epoch, num_epochs, i, total_step, loss.data[0], np.exp(loss.data[0]))) \n",
    "\n",
    "        # Save the models\n",
    "        if (i+1) % save_step == 0:\n",
    "            torch.save(dedroid_rnn.state_dict(), \n",
    "                       os.path.join(model_path, 'DeDroidRNN-%d-%d.pkl' % (epoch+1, i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_by_global_name(node_global_name):\n",
    "    # Temporaral work around\n",
    "    node_global_name = node_global_name[16:]\n",
    "    kgid = node_to_kgids[node_global_name]\n",
    "    return kgid_embeddings[kgid]\n",
    "    \n",
    "def predict_names_for_embeddings(embeddings):\n",
    "    embeddings = torch.Tensor(embeddings)\n",
    "    embeddings = to_var(embeddings, volatile=True)\n",
    "\n",
    "    # Generate names from embeddings\n",
    "    sampled_ids = dedroid_rnn.sample(embeddings)\n",
    "    print(sampled_ids.size())\n",
    "    sampled_ids = sampled_ids.cpu().data.numpy().tolist()\n",
    "\n",
    "    predicted_names = []\n",
    "    for predicted_char_ids in sampled_ids:\n",
    "        # Decode word_ids to name\n",
    "        predicted_chars = []\n",
    "        for word_id in predicted_char_ids:\n",
    "            word = vocab.idx2word[word_id]\n",
    "            if word == WORD_START or word == WORD_UNKNOWN:\n",
    "                continue\n",
    "            elif word == '<end>':\n",
    "                break\n",
    "            predicted_chars.append(word)\n",
    "        predicted_name = ''.join(predicted_chars)\n",
    "        predicted_names.append(predicted_name)\n",
    "    return predicted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick a name to predict\n",
    "global_name_to_original_names = test_dergs[0].get_kg_mappings()\n",
    "global_name, original_name = random.sample(global_name_to_original_names, 1)[0]\n",
    "print(\"predicting for %s, original name is %s\" % (global_name, original_name))\n",
    "embedding = get_embedding_by_global_name(global_name)\n",
    "predicted_name = predict_names_for_embeddings([embedding])[0]\n",
    "print(\"predicted name is %s\" % predicted_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of nodes for prediction\n",
    "global_name_to_names = []\n",
    "for g in test_dergs[20:80]:\n",
    "    global_name_to_names.extend(g.get_kg_mappings())\n",
    "global_names, original_names = zip(*global_name_to_names)\n",
    "print(\"predicting for %d nodes, original names are %s, ...\" % (len(global_names), \", \".join(original_names[:5])))\n",
    "embeddings = [get_embedding_by_global_name(global_name) for global_name in global_names]\n",
    "predicted_names = predict_names_for_embeddings(embeddings)\n",
    "print(\"predicted %d names, predicted names are %s\" % (len(predicted_names), \", \".join(predicted_names[:5])))\n",
    "num_correct = sum([1 if origin == predict else 0 for origin, predict in zip(original_names, predicted_names)])\n",
    "print(\"correctly predicted %d names, accuracy is %.2f.\" % (num_correct, float(num_correct)/len(global_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 17821 nodes for clustering, original names are Loader, android, support, v4, content, ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1195, 1698, 1698, ...,  618,   48,  910], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using K-means clustering to check the dataset.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "global_name_to_names = []\n",
    "for g in train_dergs[20:30]:\n",
    "    global_name_to_names.extend(g.get_kg_mappings())\n",
    "global_names, original_names = zip(*global_name_to_names)\n",
    "print(\"using %d nodes for clustering, original names are %s, ...\" % (len(global_names), \", \".join(original_names[:5])))\n",
    "embeddings = [get_embedding_by_global_name(global_name) for global_name in global_names]\n",
    "X = np.array(embeddings)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute K-Means\n",
    "kmeans = KMeans(n_clusters=1000, random_state=0).fit(X)\n",
    "label_list = kmeans.labels_.tolist()\n",
    "\n",
    "def get_label_names(label):\n",
    "    for i in range(len(original_names)):\n",
    "        if label_list[i] == label:\n",
    "            print(original_names[i])\n",
    "get_label_names(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getCallback\n",
      "access$100\n",
      "getStartActivityDelegate\n",
      "getAuthorizationClientRequest\n",
      "statusCallback\n",
      "access$1000\n",
      "setCallback\n",
      "setDefaultAudience\n",
      "setLoginBehavior\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
