{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"No input file specified.\")? (utils.py, line 49)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/liyc/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e6ff99226258>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from utils import load_dergs\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/liyc/projects/DeDroid/scripts/utils.py\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    print \"No input file specified.\"\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"No input file specified.\")?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "\n",
    "from utils import load_dergs\n",
    "from derg import DERG, KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dergs = load_dergs('/home/liyc/data/dedroid/dergs/', 'obfuscate_derg.json')\n",
    "print(len(dergs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(name):\n",
    "    \"\"\"\n",
    "    split an element name to words\n",
    "    the name could be:\n",
    "    camel case: mMyActivity, MyAwesomeClass\n",
    "    underscore-splited: MY_AWESOME_FIELD, parse_names\n",
    "    hybrid: create_myAwesomeClass_COMPAT\n",
    "    \"\"\"\n",
    "    words = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z]|$|_)|<init>|<clinit>|\\$', name)\n",
    "    return map(lambda word: str.lower(str(word)), words)\n",
    "\n",
    "print(parse_name('mMyActivity'))\n",
    "print(parse_name('MyAwesomeClass'))\n",
    "print(parse_name('MY_AWESOME_FIELD'))\n",
    "print(parse_name('AWESOMEMethod'))\n",
    "print(parse_name('parse_names'))\n",
    "print(parse_name('create_myAwesomeClass_COMPAT'))\n",
    "print(parse_name('create_myAwesomeClass$SubClass$access1000'))\n",
    "print(parse_name('<init>'))\n",
    "print(parse_name('x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_START = '<start>'\n",
    "WORD_END = '<end>'\n",
    "WORD_UNKNOWN = '<unknown>'\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx[WORD_UNKNOWN]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.add_word(WORD_START)\n",
    "vocab.add_word(WORD_END)\n",
    "vocab.add_word(WORD_UNKNOWN)\n",
    "\n",
    "node_to_names = {}\n",
    "for derg in dergs:\n",
    "    node_to_names.update(derg.get_kg_mappings())\n",
    "\n",
    "words = []\n",
    "for name in node_to_names.values():\n",
    "    words.extend(parse_name(name))\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(words)\n",
    "\n",
    "for word in counter:\n",
    "    if counter[word] > 2:\n",
    "        vocab.add_word(word)\n",
    "\n",
    "print(\"Total vocabulary size: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node_to_kgid_mapping(node_to_kgid_mapping_path):\n",
    "    f = open(node_to_kgid_mapping_path)\n",
    "    mapping = {}\n",
    "    for line in f.readlines()[1:]:\n",
    "        segs = line.split()\n",
    "        node_global_name = segs[0]\n",
    "        kgid = int(segs[1])\n",
    "        mapping[node_global_name] = kgid\n",
    "    return mapping\n",
    "\n",
    "node_to_kgids = load_node_to_kgid_mapping('/home/liyc/data/dedroid/kg_without3lib/entity2id.txt')\n",
    "print(len(node_to_kgids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kgid_embeddings(kgid_embedding_path):\n",
    "    embeddings = json.load(open(kgid_embedding_path))\n",
    "    return embeddings['ent_embeddings']\n",
    "\n",
    "kgid_embeddings = load_kgid_embeddings('/home/liyc/data/dedroid/kg_without3lib/embedding_TransE_64/embedding.vec.json')\n",
    "print(len(kgid_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_train_and_test(dergs, train_portion = 0.9):\n",
    "    random.shuffle(dergs)\n",
    "    sep = int(len(dergs) * train_portion)\n",
    "    train_dergs = dergs[:sep]\n",
    "    test_dergs = dergs[sep:]\n",
    "    return train_dergs, test_dergs\n",
    "\n",
    "train_dergs, test_dergs = split_train_and_test(dergs)\n",
    "print(\"%d apps for training, %d apps for predicting\" % (len(train_dergs), len(test_dergs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeDroidDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset compatible with torch.utils.data.DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, dergs, node_to_kgids, kgid_embeddings, vocab):\n",
    "        self.dergs = dergs\n",
    "        self.node_to_names = []\n",
    "        for g in dergs:\n",
    "            self.node_to_names.extend(g.get_kg_mappings())\n",
    "        self.node_to_kgids = node_to_kgids\n",
    "        self.kgid_embeddings = kgid_embeddings\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (embedding and name).\"\"\"\n",
    "        node_global_name, node_name = self.node_to_names[index]\n",
    "        \n",
    "        # Temporaral work around\n",
    "        node_global_name = node_global_name[24:]\n",
    "        \n",
    "        kgid = self.node_to_kgids[node_global_name]\n",
    "        embedding = self.kgid_embeddings[kgid]\n",
    "\n",
    "        # Convert node name (string) to word ids.\n",
    "        name_vec = []\n",
    "        name_vec.append(vocab(WORD_START))\n",
    "        name_vec.extend(map(vocab, parse_name(node_name)))\n",
    "        name_vec.append(vocab(WORD_END))\n",
    "        \n",
    "        embedding = torch.Tensor(embedding)\n",
    "        target = torch.Tensor(name_vec)\n",
    "        return embedding, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.node_to_names)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (embedding, name).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging name (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (embedding, name). \n",
    "            - embedding: torch tensor of shape (embed_size).\n",
    "            - name: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        embeddings: torch tensor of shape (batch_size, embed_size).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded name.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    embeddings, names = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 1D tensor to 2D tensor).\n",
    "    embeddings = torch.stack(embeddings, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(name) for name in names]\n",
    "    targets = torch.zeros(len(names), max(lengths)).long()\n",
    "    for i, name in enumerate(names):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = name[:end]\n",
    "    return embeddings, targets, lengths\n",
    "\n",
    "\n",
    "def get_loader(dergs, node_to_kgids, kgid_embeddings, vocab, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom DeDroid dataset.\"\"\"\n",
    "    # DeDroid dataset\n",
    "    dedroid_data = DeDroidDataset(dergs, node_to_kgids, kgid_embeddings, vocab)\n",
    "    \n",
    "    # Data loader for DeDroid dataset\n",
    "    # This will return (embeddings, names, lengths) for every iteration.\n",
    "    # embeddings: tensor of shape (batch_size, embed_size).\n",
    "    # names: tensor of shape (batch_size, padded_length).\n",
    "    # lengths: list indicating valid length for each name. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dedroid_data, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "    \n",
    "    \n",
    "class DeDroidRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DeDroidRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, embeddings, names, lengths):\n",
    "        \"\"\"Decode kg embeddings and generate names.\"\"\"\n",
    "        name_embeddings = self.embed(names)\n",
    "        embeddings = torch.cat((embeddings.unsqueeze(1), name_embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, embeddings, states=None):\n",
    "        \"\"\"Samples names for given kg embeddings (Greedy search).\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = embeddings.unsqueeze(1)\n",
    "        for i in range(10):                                      # maximum sampling length\n",
    "            hiddens, states = self.lstm(inputs, states)          # (batch_size, 1, hidden_size), \n",
    "            outputs = self.linear(hiddens.squeeze(1))            # (batch_size, vocab_size)\n",
    "            predicted = outputs.max(1)[1]\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)                         # (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # (batch_size, 20)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "shuffle = True\n",
    "num_workers = 8\n",
    "embed_size = 64\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "lr = 0.006\n",
    "num_epochs = 20\n",
    "log_step = 500\n",
    "save_step = 10000\n",
    "model_path = '/home/liyc/data/dedroid/model'\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "def train_with_dergs(train_dergs):\n",
    "    # Build data loader\n",
    "    data_loader = get_loader(\n",
    "        dergs = train_dergs,\n",
    "        node_to_kgids = node_to_kgids,\n",
    "        kgid_embeddings = kgid_embeddings,\n",
    "        vocab = vocab,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        num_workers = num_workers) \n",
    "\n",
    "    # Build the models\n",
    "    dedroid_rnn = DeDroidRNN(\n",
    "        embed_size = embed_size,\n",
    "        hidden_size = hidden_size,\n",
    "        vocab_size = len(vocab),\n",
    "        num_layers = num_layers,\n",
    "        dropout = dropout\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        dedroid_rnn.cuda()\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = list(dedroid_rnn.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    # Train the Models\n",
    "    total_step = len(data_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (embeddings, names, lengths) in enumerate(data_loader):\n",
    "            # Set mini-batch dataset\n",
    "            embeddings = to_var(embeddings, volatile=False)\n",
    "            names = to_var(names)\n",
    "            targets = pack_padded_sequence(names, lengths, batch_first=True)[0]\n",
    "\n",
    "            # Forward, Backward and Optimize\n",
    "            dedroid_rnn.zero_grad()\n",
    "            outputs = dedroid_rnn(embeddings, names, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print log info\n",
    "            if i % log_step == 0:\n",
    "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f'\n",
    "                      % (epoch+1, num_epochs, i, total_step, loss.data[0], np.exp(loss.data[0]))) \n",
    "\n",
    "            # Save the models\n",
    "            if (i+1) % save_step == 0:\n",
    "                torch.save(dedroid_rnn.state_dict(), \n",
    "                           os.path.join(model_path, 'DeDroidRNN-%d-%d.pkl' % (epoch+1, i+1)))\n",
    "                \n",
    "    return dedroid_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_embedding(node_global_id):\n",
    "    # Temporaral work around\n",
    "    node_global_id = node_global_id[24:]\n",
    "    kgid = node_to_kgids[node_global_id]\n",
    "    return kgid_embeddings[kgid]\n",
    "    \n",
    "def predict_with_embeddings(dedroid_rnn, embeddings):\n",
    "    embeddings = torch.Tensor(embeddings)\n",
    "    embeddings = to_var(embeddings, volatile=True)\n",
    "\n",
    "    # Generate names from embeddings\n",
    "    sampled_ids = dedroid_rnn.sample(embeddings)\n",
    "    print(sampled_ids.size())\n",
    "    sampled_ids = sampled_ids.cpu().data.numpy().tolist()\n",
    "\n",
    "    predicted_names = []\n",
    "    for predicted_char_ids in sampled_ids:\n",
    "        # Decode word_ids to name\n",
    "        predicted_chars = []\n",
    "        for word_id in predicted_char_ids:\n",
    "            word = vocab.idx2word[word_id]\n",
    "            if word == WORD_START or word == WORD_UNKNOWN:\n",
    "                continue\n",
    "            elif word == '<end>':\n",
    "                break\n",
    "            predicted_chars.append(word)\n",
    "        predicted_name = '_'.join(predicted_chars)\n",
    "        predicted_names.append(predicted_name)\n",
    "    return predicted_names\n",
    "\n",
    "def predict_random(dedroid_rnn, predict_dergs):\n",
    "    # randomly pick a name to predict\n",
    "    node_gid_to_names = []\n",
    "    for g in predict_dergs:\n",
    "        node_gid_to_names.extend(g.get_kg_mappings())\n",
    "    node_gid, original_name = random.sample(node_gid_to_names, 1)[0]\n",
    "    print(\"predicting for %s, original name is %s\" % (node_gid, original_name))\n",
    "    embedding = get_node_embedding(node_gid)\n",
    "    predicted_name = predict_with_embeddings(dedroid_rnn, [embedding])[0]\n",
    "    print(\"predicted name is %s\" % predicted_name)\n",
    "\n",
    "def predict_with_dergs(dedroid_rnn, predict_dergs):\n",
    "    # get a list of nodes for prediction\n",
    "    node_gid_to_names = []\n",
    "    for g in predict_dergs:\n",
    "        node_gid_to_names.extend(g.get_kg_mappings())\n",
    "    node_gids, original_names = zip(*node_gid_to_names)\n",
    "    print(\"predicting for %d nodes, original names are %s, ...\" % (len(node_gids), \", \".join(original_names[:5])))\n",
    "    embeddings = [get_node_embedding(node_gid) for node_gid in node_gids]\n",
    "    predicted_names = predict_with_embeddings(dedroid_rnn, embeddings)\n",
    "    print(\"predicted %d names, predicted names are %s, ...\" % (len(predicted_names), \", \".join(predicted_names[:5])))\n",
    "#     num_correct = sum([1 if origin == predict else 0 for origin, predict in zip(original_names, predicted_names)])\n",
    "#     print(\"correctly predicted %d names, accuracy is %.2f.\" % (num_correct, float(num_correct)/len(global_names)))\n",
    "    return zip(node_gids, original_names, predicted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def batch_train_predict(dergs, n_fold = 10):\n",
    "    splited_dergs = np.array_split(dergs, n_fold)\n",
    "    predictions = []\n",
    "    for i in range(n_fold):\n",
    "        print(\"Fold %d\" % i)\n",
    "        predict_dergs = list(splited_dergs[i])\n",
    "        train_dergs = list(itertools.chain(*(splited_dergs[:i]+splited_dergs[i+1:])))\n",
    "        print(\"%d dergs for training, %d dergs for predicting\" % (len(train_dergs), len(predict_dergs)))\n",
    "        dedroid_rnn = train_with_dergs(train_dergs)\n",
    "        predictions.extend(predict_with_dergs(dedroid_rnn, predict_dergs))\n",
    "    return predictions\n",
    "\n",
    "predictions = batch_train_predict(dergs)\n",
    "json.dump(predictions, open('/home/liyc/data/dedroid/predictions.json', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(s1, s2):\n",
    "    m=len(s1)+1\n",
    "    n=len(s2)+1\n",
    "\n",
    "    tbl = {}\n",
    "    for i in range(m): tbl[i,0]=i\n",
    "    for j in range(n): tbl[0,j]=j\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            tbl[i,j] = min(tbl[i, j-1]+1, tbl[i-1, j]+1, tbl[i-1, j-1]+cost)\n",
    "\n",
    "    return tbl[i,j]\n",
    "\n",
    "def similarity(original_name, predicted_name):\n",
    "    original_words = parse_name(original_name)\n",
    "    predicted_words = parse_name(predicted_name)\n",
    "    distance = edit_distance(original_words, predicted_words)\n",
    "    if distance == 0:\n",
    "        return 1.0\n",
    "    return 1 - float(distance) / max(len(original_words), len(predicted_words))\n",
    "\n",
    "print(similarity(\"Hello_world\", \"HelloWorldHAHA\")) # should be 0.666\n",
    "print(similarity(\"a\", \"b\")) # should be 0\n",
    "print(similarity(\"getData\", \"get\")) # should be 0.5\n",
    "print(similarity(\"\", \"\")) # should be 1\n",
    "\n",
    "def score(original_name, predicted_name):\n",
    "    s = similarity(original_name, predicted_name)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    results = []\n",
    "    prediction_dict = dict([(gid, predict) for gid, origin, predict in predictions])\n",
    "    for derg in dergs:\n",
    "        for node in derg.nodes():\n",
    "            node_type = node['type']\n",
    "            node_name = node['name']\n",
    "            if node_type in ['class', 'method', 'field'] and len(node_name) < 3:\n",
    "                original_name = node['original_name'] if 'original_name' in node else ''\n",
    "                deguard_name = node['deguard_name'] if 'deguard_name' in node else ''\n",
    "                if len(original_name) < 3:\n",
    "                    continue\n",
    "                deguard_score = score(original_name, deguard_name)\n",
    "                node_gid = derg.get_node_global_id(node)\n",
    "                dedroid_name = prediction_dict[node_gid] if node_gid in prediction_dict else ''\n",
    "                dedroid_score = score(original_name, dedroid_name)\n",
    "                results.append((node_type, original_name, node_name, deguard_name, deguard_score, dedroid_name, dedroid_score))\n",
    "    return results\n",
    "\n",
    "results = evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))\n",
    "deguard_average_score = np.average(zip(*results)[4])\n",
    "dedroid_average_score = np.average(zip(*results)[6])\n",
    "print(deguard_average_score, dedroid_average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using K-means clustering to check the dataset.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "global_name_to_names = []\n",
    "for g in dergs[:50]:\n",
    "    global_name_to_names.extend(g.get_kg_mappings())\n",
    "global_names, original_names = zip(*global_name_to_names)\n",
    "print(\"using %d nodes for clustering, original names are %s, ...\" % (len(global_names), \", \".join(original_names[:5])))\n",
    "embeddings = [get_embedding_by_global_name(global_name) for global_name in global_names]\n",
    "X = np.array(embeddings)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute K-Means\n",
    "kmeans = KMeans(n_clusters=1000, random_state=0).fit(X)\n",
    "label_list = kmeans.labels_.tolist()\n",
    "\n",
    "def get_label_names(label):\n",
    "    for i in range(len(original_names)):\n",
    "        if label_list[i] == label:\n",
    "            print(original_names[i])\n",
    "get_label_names(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
